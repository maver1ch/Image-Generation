# Anime Face Synthesis using DCGAN, VAE, and Denoising Diffusion Models

This project explores the implementation of three generative models—DCGAN, VAE, and Denoising Diffusion Models—for anime face generation. Using the PyTorch framework, I enhanced models through experimentation with advanced techniques such as U-Net architecture, Attention Mechanism, Squeeze-and-Excitation, and Residual Blocks. The performance of the models is evaluated based on image quality.

## About The Project

This project aims to generate high-quality anime faces using different generative models, each with its own strengths and weaknesses. 

## Main Models Implemented:
- **DCGAN (Deep Convolutional GAN)**: A generative adversarial network specifically designed to generate anime faces from noise.
- **Variational Autoencoder (VAE)**: A latent variable model used to generate anime faces by learning the underlying distribution of the data.
- **Denoising Diffusion Probabilistic Model (DDPM)**: A generative model that iteratively denoises noisy data to generate high-quality images.

## Enhancements and Techniques:
- **U-Net Architecture**: Incorporated for enhanced performance in image segmentation and generation tasks.
- **Attention Mechanism**: Applied to improve the model’s focus on important features, leading to better image quality.
- **Squeeze-and-Excitation**: Used to recalibrate channel-wise feature responses for more accurate generation.
- **Residual Blocks**: Added to allow the models to train deeper without vanishing gradients and improve image consistency.

## Limitations and Future Work

### Current Limitations:
- **Blurry Images in VAE**: The images generated by the VAE tend to be blurry, which affects the overall visual quality compared to the other models.
- **Training Time**: Denoising Diffusion Models require significantly more training time compared to DCGAN and VAE.
- **Dataset Complexity for DDPM**: The dataset is not complex enough for the Denoising Diffusion Models (DDPM), resulting in lower-quality images compared to the other models.

### Future Improvements:
- **Conditional Generation**: Extending the models to conditional generation, allowing users to input desired features (e.g., hair color, eye color).
- **Improved Training Efficiency**: Optimizing the training pipeline to reduce the time required for Denoising Diffusion Models.
